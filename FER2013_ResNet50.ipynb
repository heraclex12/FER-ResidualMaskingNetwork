{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FER2013 Classification (3 channels)",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1WPZW4ts4iwHAUCWCJU88mzUIK8NdDzSz",
      "authorship_tag": "ABX9TyPllO1jP1aNj8WaHQOE7boZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heraclex12/FER-ResidualMaskingNetwork/blob/master/FER2013_ResNet50.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDXI5Zvblu2f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4ce4112a-8ce5-48f1-9338-35fc4f63464b"
      },
      "source": [
        "import warnings\n",
        "\n",
        "from keras.layers import Input\n",
        "from keras import layers\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import AveragePooling2D\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "from keras.layers import GlobalMaxPooling2D\n",
        "from keras.layers import BatchNormalization, Lambda, Add, Multiply\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau, ModelCheckpoint\n",
        "from keras import backend as K\n",
        "from keras.engine.topology import get_source_inputs\n",
        "from keras.utils import layer_utils, to_categorical\n",
        "from keras.utils.data_utils import get_file\n",
        "from tensorflow.image import resize\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels.h5'\n",
        "WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1cca7tXzV4x",
        "colab_type": "text"
      },
      "source": [
        "Some solution for convert 1channel into 3 channels\n",
        "\n",
        "First, as below, using numpy repeat built-in function: x = np.repeat(x, 3, -1)\n",
        "\n",
        "Second, I haven't try it yet, but I saw it in kaggle, just try.\n",
        "\n",
        "- <code>img_input = Input(shape=(img_size_target,img_size_target,1)) </br>\n",
        "img_conc = Concatenate()([img_input, img_input, img_input])   </code>\n",
        "\n",
        "Third, it depend on which library you use, but main step is you must exclude first convolutional layer'pretrained model. in Keras, you put exclude=['conv1'] on load_weights statement and initialize random weight (better than, average it) for it; in Pytorch, you get conv1 weight ouput and sum it into 1-dimension\n",
        "\n",
        "Fourth, you can use ImageDataGenerator in Keras to load grayscale as RGB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZ7-tslsFEFC",
        "colab_type": "text"
      },
      "source": [
        "### RESNET50 model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCTzc3MZkv0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def identity_block(input_tensor, kernel_size, filters, stage, block):\n",
        "    filters1, filters2, filters3 = filters\n",
        "    bn_axis = 3\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "\n",
        "    x = Conv2D(filters1, (1, 1), name=conv_name_base + '2a')(input_tensor)\n",
        "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Conv2D(filters2, kernel_size,\n",
        "               padding='same', name=conv_name_base + '2b')(x)\n",
        "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Conv2D(filters3, (1, 1), name=conv_name_base + '2c')(x)\n",
        "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
        "\n",
        "    x = Add()([x, input_tensor])\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2)):\n",
        "\n",
        "    filters1, filters2, filters3 = filters\n",
        "    bn_axis = 3\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "\n",
        "    x = Conv2D(filters1, (1, 1), strides=strides,\n",
        "               name=conv_name_base + '2a')(input_tensor)\n",
        "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Conv2D(filters2, kernel_size, padding='same',\n",
        "               name=conv_name_base + '2b')(x)\n",
        "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Conv2D(filters3, (1, 1), name=conv_name_base + '2c')(x)\n",
        "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
        "\n",
        "    shortcut = Conv2D(filters3, (1, 1), strides=strides,\n",
        "                      name=conv_name_base + '1')(input_tensor)\n",
        "    shortcut = BatchNormalization(axis=bn_axis, name=bn_name_base + '1')(shortcut)\n",
        "\n",
        "    x = Add()([x, shortcut])\n",
        "    x = Activation('relu')(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBsgt-epk5xV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ResNet50(weights='imagenet',\n",
        "             input_tensor=None, input_shape=None,\n",
        "             pooling=None,\n",
        "             classes=1000):\n",
        "  \n",
        "    if input_tensor is None:\n",
        "        img_input = Input(shape=input_shape)\n",
        "    else:\n",
        "        if not K.is_keras_tensor(input_tensor):\n",
        "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
        "        else:\n",
        "            img_input = input_tensor\n",
        "\n",
        "    bn_axis = 3\n",
        "\n",
        "    x = Lambda(lambda img : tf.image.resize(img, (224, 224)))(img_input)\n",
        "    x = Conv2D(64, (7, 7), strides=(2, 2), padding='same', name='conv1')(x)\n",
        "    x = BatchNormalization(axis=bn_axis, name='bn_conv1')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling2D((3, 3), strides=(2, 2), padding=\"same\")(x)\n",
        "\n",
        "    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1))\n",
        "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b')\n",
        "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='c')\n",
        "\n",
        "    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a')\n",
        "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b')\n",
        "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c')\n",
        "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='d')\n",
        "\n",
        "    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a')\n",
        "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='b')\n",
        "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='c')\n",
        "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='d')\n",
        "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='e')\n",
        "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='f')\n",
        "\n",
        "    x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a')\n",
        "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b')\n",
        "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c')\n",
        "\n",
        "        # AVGPOOL\n",
        "    x = AveragePooling2D(pool_size=(2,2), padding='same')(x)\n",
        "\n",
        "    # Output layer\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(classes, activation='softmax', name='fc' + str(classes))(x)\n",
        "\n",
        "    if input_tensor is not None:\n",
        "        inputs = get_source_inputs(input_tensor)\n",
        "    else:\n",
        "        inputs = img_input\n",
        "    # Create model.\n",
        "    model = Model(inputs, x, name='resnet50')\n",
        "\n",
        "    # load weights\n",
        "    weights_path = get_file('resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
        "                                    WEIGHTS_PATH_NO_TOP,\n",
        "                                    cache_subdir='models',\n",
        "                                    md5_hash='a268eb855778b3df3c7506639542a6af')\n",
        "    # weights_path = \"drive/My Drive/fer2013_data/models/weights.h5\"\n",
        "    model.load_weights(weights_path,by_name=True)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0MWWwREFB1H",
        "colab_type": "text"
      },
      "source": [
        "### LOAD DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVicUODglp1J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_set = pd.read_csv(\"drive/My Drive/fer2013_data/train.csv\")\n",
        "test_set = pd.read_csv(\"drive/My Drive/fer2013_data/test.csv\")\n",
        "validation_set = pd.read_csv(\"drive/My Drive/fer2013_data/val.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XjMuUc_mEm3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_labels = to_categorical(training_set['emotion'])\n",
        "test_labels = to_categorical(test_set['emotion'])\n",
        "validation_labels = to_categorical(validation_set['emotion'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLPITMFWmNBM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_pixels = training_set['pixels'].str.split().tolist()\n",
        "training_pixels = np.array(training_pixels)\n",
        "training_pixels = training_pixels.reshape(-1, 48, 48, 1)\n",
        "training_pixels = np.repeat(training_pixels, 3, -1)        # convert to 3channels\n",
        "training_pixels = training_pixels.astype(\"float32\") / 255\n",
        "\n",
        "test_pixels = test_set['pixels'].str.split().tolist()\n",
        "test_pixels = np.array(test_pixels)\n",
        "test_pixels = test_pixels.reshape(-1, 48, 48, 1)\n",
        "test_pixels = np.repeat(test_pixels, 3, -1)        # convert to 3channels\n",
        "test_pixels = test_pixels.astype(\"float32\") / 255\n",
        "\n",
        "validation_pixels = validation_set['pixels'].str.split().tolist()\n",
        "validation_pixels = np.array(validation_pixels)\n",
        "validation_pixels = validation_pixels.reshape(-1, 48, 48, 1)\n",
        "validation_pixels = np.repeat(validation_pixels, 3, -1)        # convert to 3channels\n",
        "validation_pixels = validation_pixels.astype(\"float32\") / 255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXyAETxO1Ykj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1, min_lr = 1e-8)\n",
        "\n",
        "early_stopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=8, verbose=1, mode='auto')\n",
        "\n",
        "tensorBoard = TensorBoard(log_dir='drive/My Drive/fer2013_data/logs')\n",
        "\n",
        "checkpointer = ModelCheckpoint(\"drive/My Drive/fer2013_data/models/resnet50_with_pretrained_model.h5\", monitor='val_loss', verbose=1, save_best_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBdYQ7o3FIVL",
        "colab_type": "text"
      },
      "source": [
        "### Freeze and training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fW5Psm3dk6GH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "f02c217f-66d3-4c12-bf1a-b53bfe548080"
      },
      "source": [
        "checkpointer = ModelCheckpoint(\"drive/My Drive/fer2013_data/models/resnet50_with_pretrained_model.h5\", monitor='val_accuracy', verbose=1, save_best_only=True)\n",
        "\n",
        "model = ResNet50(input_shape=(48, 48, 3), classes=7)\n",
        "\n",
        "for layer in model.layers[:-1]:\n",
        "  layer.trainable = False\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1, min_lr = 1e-8)\n",
        "model.fit(training_pixels, training_labels, batch_size = 48, epochs=10, validation_data= (validation_pixels, validation_labels),\n",
        "          callbacks=[lr_reducer, checkpointer])\n",
        "\n",
        "model.load_weights(\"drive/My Drive/fer2013_data/models/resnet50_with_pretrained_model.h5\")\n",
        "score, acc = model.evaluate(validation_pixels, validation_labels, verbose=0)\n",
        "print(\"Accuracy:\", acc)\n",
        "print(\"Score:\", score)\n",
        "score, acc = model.evaluate(test_pixels, test_labels, verbose=0)\n",
        "print(\"Accuracy:\", acc)\n",
        "print(\"Score:\", score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 28709 samples, validate on 3589 samples\n",
            "Epoch 1/10\n",
            "28709/28709 [==============================] - 89s 3ms/step - loss: 3.4183 - accuracy: 0.4403 - val_loss: 4.5966 - val_accuracy: 0.1301\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.13012, saving model to drive/My Drive/fer2013_data/models/resnet50_with_pretrained_model.h5\n",
            "Epoch 2/10\n",
            "28709/28709 [==============================] - 85s 3ms/step - loss: 2.1430 - accuracy: 0.6246 - val_loss: 6.1527 - val_accuracy: 0.0156\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.13012\n",
            "Epoch 3/10\n",
            "28709/28709 [==============================] - 85s 3ms/step - loss: 1.5405 - accuracy: 0.7123 - val_loss: 6.5707 - val_accuracy: 0.1934\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.13012 to 0.19337, saving model to drive/My Drive/fer2013_data/models/resnet50_with_pretrained_model.h5\n",
            "Epoch 4/10\n",
            "28709/28709 [==============================] - 86s 3ms/step - loss: 1.2256 - accuracy: 0.7640 - val_loss: 9.1252 - val_accuracy: 0.1301\n",
            "\n",
            "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.19337\n",
            "Epoch 5/10\n",
            "28709/28709 [==============================] - 86s 3ms/step - loss: 0.5480 - accuracy: 0.8650 - val_loss: 6.9435 - val_accuracy: 0.1382\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.19337\n",
            "Epoch 6/10\n",
            "28709/28709 [==============================] - 86s 3ms/step - loss: 0.2959 - accuracy: 0.9126 - val_loss: 7.1657 - val_accuracy: 0.1382\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.19337\n",
            "Epoch 7/10\n",
            "28709/28709 [==============================] - 86s 3ms/step - loss: 0.2655 - accuracy: 0.9202 - val_loss: 7.9401 - val_accuracy: 0.1382\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.19337\n",
            "Epoch 8/10\n",
            "28709/28709 [==============================] - 86s 3ms/step - loss: 0.1407 - accuracy: 0.9561 - val_loss: 7.1882 - val_accuracy: 0.1402\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.19337\n",
            "Epoch 9/10\n",
            "28709/28709 [==============================] - 86s 3ms/step - loss: 0.1066 - accuracy: 0.9656 - val_loss: 9.0193 - val_accuracy: 0.1382\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.19337\n",
            "Epoch 10/10\n",
            "28709/28709 [==============================] - 86s 3ms/step - loss: 0.1007 - accuracy: 0.9676 - val_loss: 8.7830 - val_accuracy: 0.1382\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.19337\n",
            "Accuracy: 0.19336862862110138\n",
            "Score: 6.570674736940219\n",
            "Accuracy: 0.19169685244560242\n",
            "Score: 6.509062119000601\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UefsA7PFLT3",
        "colab_type": "text"
      },
      "source": [
        "### Unfreeze and training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n751l5enhdCy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "73571701-2d64-404e-8174-53b98b80da22"
      },
      "source": [
        "checkpointer = ModelCheckpoint(\"drive/My Drive/fer2013_data/models/resnet50_with_pretrained_model.h5\", monitor='val_accuracy', verbose=1, save_best_only=True)\n",
        "\n",
        "for layer in model.layers[:-1]:\n",
        "  layer.trainable = True\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1, min_lr = 1e-8)\n",
        "\n",
        "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(training_pixels, training_labels, batch_size = 48, epochs=50, validation_data= (validation_pixels, validation_labels),\n",
        "          callbacks=[lr_reducer, checkpointer])\n",
        "\n",
        "model.load_weights(\"drive/My Drive/fer2013_data/models/resnet50_with_pretrained_model.h5\")\n",
        "score, acc = model.evaluate(test_pixels, test_labels, verbose=0)\n",
        "print(\"Accuracy:\", acc)\n",
        "print(\"Score:\", score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 28709 samples, validate on 3589 samples\n",
            "Epoch 1/50\n",
            "28709/28709 [==============================] - 315s 11ms/step - loss: 1.8522 - accuracy: 0.4743 - val_loss: 1.8918 - val_accuracy: 0.1808\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.18083, saving model to drive/My Drive/fer2013_data/models/resnet50_with_pretrained_model.h5\n",
            "Epoch 2/50\n",
            "28709/28709 [==============================] - 299s 10ms/step - loss: 1.2270 - accuracy: 0.5600 - val_loss: 1.5841 - val_accuracy: 0.3670\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.18083 to 0.36695, saving model to drive/My Drive/fer2013_data/models/resnet50_with_pretrained_model.h5\n",
            "Epoch 3/50\n",
            "28709/28709 [==============================] - 299s 10ms/step - loss: 1.3152 - accuracy: 0.5570 - val_loss: 3.1884 - val_accuracy: 0.1900\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.36695\n",
            "Epoch 4/50\n",
            "28709/28709 [==============================] - 299s 10ms/step - loss: 1.3560 - accuracy: 0.5332 - val_loss: 1.1913 - val_accuracy: 0.5514\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.36695 to 0.55141, saving model to drive/My Drive/fer2013_data/models/resnet50_with_pretrained_model.h5\n",
            "Epoch 5/50\n",
            "28709/28709 [==============================] - 299s 10ms/step - loss: 0.9862 - accuracy: 0.6337 - val_loss: 1.1036 - val_accuracy: 0.5938\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.55141 to 0.59376, saving model to drive/My Drive/fer2013_data/models/resnet50_with_pretrained_model.h5\n",
            "Epoch 6/50\n",
            "28709/28709 [==============================] - 299s 10ms/step - loss: 0.8776 - accuracy: 0.6753 - val_loss: 1.1560 - val_accuracy: 0.5921\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.59376\n",
            "Epoch 7/50\n",
            "28709/28709 [==============================] - 299s 10ms/step - loss: 0.8978 - accuracy: 0.6772 - val_loss: 1.2689 - val_accuracy: 0.5581\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.59376\n",
            "Epoch 8/50\n",
            "28709/28709 [==============================] - 299s 10ms/step - loss: 0.8027 - accuracy: 0.7120 - val_loss: 1.2270 - val_accuracy: 0.5815\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.59376\n",
            "Epoch 9/50\n",
            "28709/28709 [==============================] - 299s 10ms/step - loss: 0.4560 - accuracy: 0.8396 - val_loss: 1.2567 - val_accuracy: 0.6043\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.59376 to 0.60435, saving model to drive/My Drive/fer2013_data/models/resnet50_with_pretrained_model.h5\n",
            "Epoch 10/50\n",
            "28709/28709 [==============================] - 299s 10ms/step - loss: 0.5414 - accuracy: 0.8109 - val_loss: 1.4062 - val_accuracy: 0.6085\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.60435 to 0.60853, saving model to drive/My Drive/fer2013_data/models/resnet50_with_pretrained_model.h5\n",
            "Epoch 11/50\n",
            "28709/28709 [==============================] - 299s 10ms/step - loss: 0.2018 - accuracy: 0.9412 - val_loss: 1.5180 - val_accuracy: 0.6046\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.60853\n",
            "Epoch 12/50\n",
            "28709/28709 [==============================] - 299s 10ms/step - loss: 0.0656 - accuracy: 0.9874 - val_loss: 1.5939 - val_accuracy: 0.6158\n",
            "\n",
            "Epoch 00012: val_accuracy improved from 0.60853 to 0.61577, saving model to drive/My Drive/fer2013_data/models/resnet50_with_pretrained_model.h5\n",
            "Epoch 13/50\n",
            "28709/28709 [==============================] - 299s 10ms/step - loss: 0.0397 - accuracy: 0.9949 - val_loss: 1.6614 - val_accuracy: 0.6174\n",
            "\n",
            "Epoch 00013: val_accuracy improved from 0.61577 to 0.61744, saving model to drive/My Drive/fer2013_data/models/resnet50_with_pretrained_model.h5\n",
            "Epoch 14/50\n",
            "28709/28709 [==============================] - 299s 10ms/step - loss: 0.0311 - accuracy: 0.9958 - val_loss: 1.7569 - val_accuracy: 0.6174\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.61744\n",
            "Epoch 15/50\n",
            "28709/28709 [==============================] - 299s 10ms/step - loss: 0.0239 - accuracy: 0.9966 - val_loss: 1.7730 - val_accuracy: 0.6222\n",
            "\n",
            "Epoch 00015: val_accuracy improved from 0.61744 to 0.62218, saving model to drive/My Drive/fer2013_data/models/resnet50_with_pretrained_model.h5\n",
            "Epoch 16/50\n",
            "28709/28709 [==============================] - 299s 10ms/step - loss: 0.0172 - accuracy: 0.9971 - val_loss: 1.7883 - val_accuracy: 0.6208\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.62218\n",
            "Epoch 17/50\n",
            "28709/28709 [==============================] - 299s 10ms/step - loss: 0.0192 - accuracy: 0.9969 - val_loss: 1.8332 - val_accuracy: 0.6186\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.62218\n",
            "Epoch 18/50\n",
            "28709/28709 [==============================] - 299s 10ms/step - loss: 0.0159 - accuracy: 0.9968 - val_loss: 1.8506 - val_accuracy: 0.6236\n",
            "\n",
            "Epoch 00018: val_accuracy improved from 0.62218 to 0.62357, saving model to drive/My Drive/fer2013_data/models/resnet50_with_pretrained_model.h5\n",
            "Epoch 19/50\n",
            "28709/28709 [==============================] - 299s 10ms/step - loss: 0.0109 - accuracy: 0.9974 - val_loss: 1.9039 - val_accuracy: 0.6241\n",
            "\n",
            "Epoch 00019: val_accuracy improved from 0.62357 to 0.62413, saving model to drive/My Drive/fer2013_data/models/resnet50_with_pretrained_model.h5\n",
            "Epoch 20/50\n",
            "28709/28709 [==============================] - 299s 10ms/step - loss: 0.0116 - accuracy: 0.9971 - val_loss: 1.9534 - val_accuracy: 0.6272\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "\n",
            "Epoch 00020: val_accuracy improved from 0.62413 to 0.62719, saving model to drive/My Drive/fer2013_data/models/resnet50_with_pretrained_model.h5\n",
            "Epoch 21/50\n",
            "28709/28709 [==============================] - 300s 10ms/step - loss: 0.0100 - accuracy: 0.9977 - val_loss: 1.9738 - val_accuracy: 0.6283\n",
            "\n",
            "Epoch 00021: val_accuracy improved from 0.62719 to 0.62831, saving model to drive/My Drive/fer2013_data/models/resnet50_with_pretrained_model.h5\n",
            "Epoch 22/50\n",
            "28709/28709 [==============================] - 300s 10ms/step - loss: 0.0079 - accuracy: 0.9972 - val_loss: 1.9728 - val_accuracy: 0.6278\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.62831\n",
            "Epoch 23/50\n",
            "28709/28709 [==============================] - 300s 10ms/step - loss: 0.0079 - accuracy: 0.9971 - val_loss: 2.0150 - val_accuracy: 0.6241\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.62831\n",
            "Epoch 24/50\n",
            "28709/28709 [==============================] - 300s 10ms/step - loss: 0.0061 - accuracy: 0.9977 - val_loss: 2.0477 - val_accuracy: 0.6247\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.62831\n",
            "Epoch 25/50\n",
            "28709/28709 [==============================] - 300s 10ms/step - loss: 0.0056 - accuracy: 0.9977 - val_loss: 2.0651 - val_accuracy: 0.6202\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.62831\n",
            "Epoch 26/50\n",
            "28709/28709 [==============================] - 300s 10ms/step - loss: 0.0059 - accuracy: 0.9975 - val_loss: 2.0848 - val_accuracy: 0.6244\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.62831\n",
            "Epoch 27/50\n",
            "28709/28709 [==============================] - 300s 10ms/step - loss: 0.0049 - accuracy: 0.9980 - val_loss: 2.0961 - val_accuracy: 0.6247\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.62831\n",
            "Epoch 28/50\n",
            "28709/28709 [==============================] - 300s 10ms/step - loss: 0.0045 - accuracy: 0.9979 - val_loss: 2.1082 - val_accuracy: 0.6252\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.62831\n",
            "Epoch 29/50\n",
            "28709/28709 [==============================] - 300s 10ms/step - loss: 0.0046 - accuracy: 0.9978 - val_loss: 2.1264 - val_accuracy: 0.6239\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.62831\n",
            "Epoch 30/50\n",
            "28709/28709 [==============================] - 300s 10ms/step - loss: 0.0041 - accuracy: 0.9980 - val_loss: 2.1337 - val_accuracy: 0.6230\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.62831\n",
            "Epoch 31/50\n",
            "28709/28709 [==============================] - 299s 10ms/step - loss: 0.0043 - accuracy: 0.9978 - val_loss: 2.1390 - val_accuracy: 0.6244\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.62831\n",
            "Epoch 32/50\n",
            "28709/28709 [==============================] - 300s 10ms/step - loss: 0.0040 - accuracy: 0.9979 - val_loss: 2.1436 - val_accuracy: 0.6227\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.62831\n",
            "Epoch 33/50\n",
            "28709/28709 [==============================] - 300s 10ms/step - loss: 0.0037 - accuracy: 0.9983 - val_loss: 2.1397 - val_accuracy: 0.6236\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.62831\n",
            "Epoch 34/50\n",
            "28709/28709 [==============================] - 300s 10ms/step - loss: 0.0040 - accuracy: 0.9980 - val_loss: 2.1516 - val_accuracy: 0.6233\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.62831\n",
            "Epoch 35/50\n",
            "28709/28709 [==============================] - 300s 10ms/step - loss: 0.0037 - accuracy: 0.9984 - val_loss: 2.1574 - val_accuracy: 0.6244\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.62831\n",
            "Epoch 36/50\n",
            "28709/28709 [==============================] - 300s 10ms/step - loss: 0.0037 - accuracy: 0.9981 - val_loss: 2.1586 - val_accuracy: 0.6239\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.62831\n",
            "Epoch 37/50\n",
            "28709/28709 [==============================] - 300s 10ms/step - loss: 0.0038 - accuracy: 0.9981 - val_loss: 2.1582 - val_accuracy: 0.6236\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.62831\n",
            "Epoch 38/50\n",
            "28709/28709 [==============================] - 300s 10ms/step - loss: 0.0037 - accuracy: 0.9984 - val_loss: 2.1583 - val_accuracy: 0.6239\n",
            "\n",
            "Epoch 00038: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.62831\n",
            "Epoch 39/50\n",
            "28709/28709 [==============================] - 300s 10ms/step - loss: 0.0035 - accuracy: 0.9985 - val_loss: 2.1632 - val_accuracy: 0.6239\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.62831\n",
            "Epoch 40/50\n",
            "28709/28709 [==============================] - 300s 10ms/step - loss: 0.0035 - accuracy: 0.9983 - val_loss: 2.1618 - val_accuracy: 0.6250\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.62831\n",
            "Epoch 41/50\n",
            "28709/28709 [==============================] - 300s 10ms/step - loss: 0.0035 - accuracy: 0.9983 - val_loss: 2.1618 - val_accuracy: 0.6236\n",
            "\n",
            "Epoch 00041: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.62831\n",
            "Epoch 42/50\n",
            "28709/28709 [==============================] - 300s 10ms/step - loss: 0.0037 - accuracy: 0.9982 - val_loss: 2.1558 - val_accuracy: 0.6241\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.62831\n",
            "Epoch 43/50\n",
            "28709/28709 [==============================] - 300s 10ms/step - loss: 0.0035 - accuracy: 0.9983 - val_loss: 2.1661 - val_accuracy: 0.6239\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.62831\n",
            "Epoch 44/50\n",
            "28709/28709 [==============================] - 300s 10ms/step - loss: 0.0036 - accuracy: 0.9982 - val_loss: 2.1581 - val_accuracy: 0.6250\n",
            "\n",
            "Epoch 00044: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.62831\n",
            "Epoch 45/50\n",
            "28709/28709 [==============================] - 300s 10ms/step - loss: 0.0035 - accuracy: 0.9984 - val_loss: 2.1698 - val_accuracy: 0.6236\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.62831\n",
            "Epoch 46/50\n",
            "28709/28709 [==============================] - 300s 10ms/step - loss: 0.0035 - accuracy: 0.9985 - val_loss: 2.1690 - val_accuracy: 0.6230\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.62831\n",
            "Epoch 47/50\n",
            "28709/28709 [==============================] - 300s 10ms/step - loss: 0.0034 - accuracy: 0.9986 - val_loss: 2.1625 - val_accuracy: 0.6241\n",
            "\n",
            "Epoch 00047: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.62831\n",
            "Epoch 48/50\n",
            "28709/28709 [==============================] - 300s 10ms/step - loss: 0.0035 - accuracy: 0.9983 - val_loss: 2.1627 - val_accuracy: 0.6244\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.62831\n",
            "Epoch 49/50\n",
            "28709/28709 [==============================] - 300s 10ms/step - loss: 0.0036 - accuracy: 0.9984 - val_loss: 2.1638 - val_accuracy: 0.6239\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.62831\n",
            "Epoch 50/50\n",
            "28709/28709 [==============================] - 300s 10ms/step - loss: 0.0035 - accuracy: 0.9983 - val_loss: 2.1648 - val_accuracy: 0.6239\n",
            "\n",
            "Epoch 00050: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.62831\n",
            "Accuracy: 0.645862340927124\n",
            "Score: 1.9014596363799956\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abxRQR5fdc4H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.layers[1].weights[0][0][0][0]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}